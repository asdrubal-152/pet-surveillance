{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic segmentation retraining\n",
    "\n",
    "---\n",
    "\n",
    "**Author:** [rodoart](https://github.com/rodoart/)<br>\n",
    "**Date created:** 2021/07/14<br>\n",
    "**Last modified:** 2021/07/14<br>\n",
    "**Description:** \n",
    "This is an attempt to retrain the last layers of the pspnet_101_voc12 neural network, chosen because it yielded more mean UI in preliminary tests on the previous notebook. We're following this [image-segmentation-keras](https://github.com/divamgupta/image-segmentation-keras)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Path config\n",
    "\n",
    "If you want the files to be copied to another folder within the same machine you are working on, by a source path other than remote."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_SLUG = 'vigilancia_mascotas'\n",
    "NAME = 'semantic_segmentation_retraining'\n",
    "NUMBER = '1.0'\n",
    "\n",
    "NOTEBOOK_NAME = f'{NUMBER}-{PROJECT_SLUG}-{NAME}.ipynb'\n",
    "\n",
    "# COLAB\n",
    "#DRIVE_MOUNT = '/drive'\n",
    "#REMOTE_PATH = f'{DRIVE_MOUNT}/MyDrive/IA/seminario_innovacion/{PROJECT_SLUG}'\n",
    "#LOCAL_PATH = '.'\n",
    "#NOTEBOOK_PATH = f'{DRIVE_MOUNT}/MyDrive/Colab Notebooks/{NOTEBOOK_NAME}'\n",
    "\n",
    "# LOCAL\n",
    "REMOTE_PATH = '..'\n",
    "LOCAL_PATH = '..'\n",
    "DRIVE_MOUNT = ''\n",
    "NOTEBOOK_PATH = f'G:\\\\Mi unidad\\\\Colab Notebooks\\\\{NOTEBOOK_NAME}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DRIVE_MOUNT:\n",
    "    from google.colab import drive\n",
    "    drive.mount(DRIVE_MOUNT)\n",
    "    to_remote = True\n",
    "else:\n",
    "    to_remote = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Path functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# It depends on where the library that comes with this package is stored.\n",
    "sys.path.append(REMOTE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vigilancia_mascotas.utils.paths \\\n",
    "    import make_remote_copy_of_workspace_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_dir, remote_dir, update_from_remote,  update_to_remote, update_notebook =\\\n",
    "    make_remote_copy_of_workspace_functions(\n",
    "        local_path=LOCAL_PATH,\n",
    "        remote_path=REMOTE_PATH,\n",
    "        notebook_path = NOTEBOOK_PATH\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the dataset\n",
    "\n",
    "The data is already prepared. They can be downloaded like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vigilancia_mascotas.data.make_dataset import DataDownload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unity_Residential_Interiors.zip already existed!\n",
      "The directory tmp\\unity_residential_interiors already exists and isn't empty!\n",
      "The files have been moved or already exist.\n",
      "The directory data\\processed\\semantic_segmentation\\unity_residential_interiors already existed and isn't empty!\n"
     ]
    }
   ],
   "source": [
    "data_object = DataDownload(workspace=local_dir())\n",
    "data_object.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning from existing segmentation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DRIVE_MOUNT:\n",
    "    !pip install keras==2.4.3\n",
    "    !pip install tensorflow==2.4.1\n",
    "    !pip install keras_segmentation   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_segmentation.models.model_utils import transfer_weights\n",
    "from keras_segmentation.pretrained import pspnet_101_voc12\n",
    "from keras_segmentation.models.pspnet import pspnet_101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying weights \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "412it [00:01, 399.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied weights of 222 layers and skipped 1 layers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "pretrained_model = pspnet_101_voc12()\n",
    "\n",
    "new_model = pspnet_101( n_classes=256 )\n",
    "\n",
    "transfer_weights( pretrained_model , new_model  ) # transfer weights from pre-trained model to your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = data_object.dataset_processed_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correcting the label images\n",
    "\n",
    "The label images that come from the dataset do not comply with the required format, since their color information goes from 0 to 255, skipping some values. This can be seen when opening an image:\n",
    "\n",
    "![](../)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verifying training dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|â–Ž         | 25/700 [00:11<05:18,  2.12it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-dc2c753bf4b6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mtrain_images\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoinpath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'train_images'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mtrain_annotations\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoinpath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'train_labels'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mcheckpoints_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlocal_dir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'tmp'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'pspnet_101'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m )\n",
      "\u001b[1;32mc:\\Conda\\envs\\vigilancia_mascotas\\lib\\site-packages\\keras_segmentation\\train.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, train_images, train_annotations, input_height, input_width, n_classes, verify_dataset, checkpoints_path, epochs, batch_size, validate, val_images, val_annotations, val_batch_size, auto_resume_checkpoint, load_weights, steps_per_epoch, val_steps_per_epoch, gen_use_multiprocessing, ignore_zero_class, optimizer_name, do_augment, augmentation_name)\u001b[0m\n\u001b[0;32m    130\u001b[0m         verified = verify_segmentation_dataset(train_images,\n\u001b[0;32m    131\u001b[0m                                                \u001b[0mtrain_annotations\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 132\u001b[1;33m                                                n_classes)\n\u001b[0m\u001b[0;32m    133\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mverified\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Conda\\envs\\vigilancia_mascotas\\lib\\site-packages\\keras_segmentation\\data_utils\\data_loader.py\u001b[0m in \u001b[0;36mverify_segmentation_dataset\u001b[1;34m(images_path, segs_path, n_classes, show_all_errors)\u001b[0m\n\u001b[0;32m    157\u001b[0m         \u001b[0mreturn_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    158\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mim_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseg_fn\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_seg_pairs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 159\u001b[1;33m             \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mim_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    160\u001b[0m             \u001b[0mseg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseg_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    161\u001b[0m             \u001b[1;31m# Check dimensions match\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "new_model.train(\n",
    "    train_images = str(data_path.joinpath('train_images')),\n",
    "    train_annotations = str(data_path.joinpath('train_labels')),\n",
    "    checkpoints_path = str(local_dir('tmp', 'pspnet_101')) , epochs=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The notebook 1.0-vigilancia_mascotas-semantic_segmentation_retraining.ipynb has been updated in the remote folder\n"
     ]
    }
   ],
   "source": [
    "update_notebook()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.8 ('vigilancia_mascotas')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "601ef09c9bc66c05cb5add3ecbff44d038514508d2cd3dc8cd002c5ccc07f638"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
