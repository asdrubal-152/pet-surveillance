{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7-CldlRwMm9"
      },
      "source": [
        "# Semantic segmentation retraining\n",
        "\n",
        "---\n",
        "\n",
        "**Author:** [rodoart](https://github.com/rodoart/)<br>\n",
        "**Date created:** 2021/07/14<br>\n",
        "**Last modified:** 2021/07/20<br>\n",
        "**Description:** \n",
        "This is an attempt to retrain the last layers of the pspnet_101_voc12 neural network, chosen because it yielded more mean UI in preliminary tests on the previous notebook. We're following this [image-segmentation-keras](https://github.com/divamgupta/image-segmentation-keras)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nP9vrjbewMnB"
      },
      "source": [
        "## Path config\n",
        "\n",
        "If you want the files to be copied to another folder within the same machine you are working on, by a source path other than remote."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FK9ttpXkwMnC"
      },
      "outputs": [],
      "source": [
        "PROJECT_SLUG = 'vigilancia_mascotas'\n",
        "NAME = 'semantic_segmentation_retraining'\n",
        "NUMBER = '1.0'\n",
        "\n",
        "NOTEBOOK_NAME = f'{NUMBER}-{PROJECT_SLUG}-{NAME}.ipynb'\n",
        "\n",
        "USING_COLAB = True\n",
        "\n",
        "if USING_COLAB:\n",
        "    DRIVE_MOUNT = '/drive'\n",
        "    REMOTE_PATH = f'{DRIVE_MOUNT}/MyDrive/IA/seminario_innovacion/{PROJECT_SLUG}'\n",
        "    LOCAL_PATH = '.'\n",
        "    NOTEBOOK_PATH = f'{DRIVE_MOUNT}/MyDrive/Colab Notebooks/'\n",
        "\n",
        "else:\n",
        "    REMOTE_PATH = '..'\n",
        "    LOCAL_PATH = ''\n",
        "    DRIVE_MOUNT = ''\n",
        "    NOTEBOOK_PATH = f'G:\\\\Mi unidad\\\\Colab Notebooks\\\\'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ouG3mnygwMnE",
        "outputId": "45e087fd-1992-4f37-b1ec-9a68d53eba35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /drive\n"
          ]
        }
      ],
      "source": [
        "if USING_COLAB:\n",
        "    from google.colab import drive\n",
        "    drive.mount(DRIVE_MOUNT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qXBYNj-wMnE"
      },
      "source": [
        "## Path functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OJvRBDz6wMnF"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "# It depends on where the library that comes with this package is stored.\n",
        "sys.path.append(REMOTE_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XwWiyvV9wMnF"
      },
      "outputs": [],
      "source": [
        "from vigilancia_mascotas.utils.paths \\\n",
        "  import make_two_dir_function, TwoWorkspacePath"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x6wgxD9-wMnG"
      },
      "outputs": [],
      "source": [
        "path = make_two_dir_function(\n",
        "    local_workspace=LOCAL_PATH, \n",
        "    remote_workspace=REMOTE_PATH\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttvsiQ_VwMnG"
      },
      "source": [
        "## Prepare the dataset\n",
        "\n",
        "The data is already prepared. They can be downloaded like this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BQN7UAl8wMnJ"
      },
      "outputs": [],
      "source": [
        "from vigilancia_mascotas.data.make_dataset import DataDownload"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wB8ylcVLwMnK"
      },
      "outputs": [],
      "source": [
        "data_object = DataDownload(local_workspace=path().local,\n",
        "                           remote_workspace=path().remote)\n",
        "data_object.start()\n",
        "data_path = data_object.dataset_processed_path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCnbD_VPwMnN"
      },
      "source": [
        "## Fine-tuning from existing segmentation model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "qCecbt2lwMnO",
        "outputId": "7570453d-506d-4f4f-fc4d-bffdc82f2fc4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting keras==2.4.3\n",
            "  Downloading Keras-2.4.3-py2.py3-none-any.whl (36 kB)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras==2.4.3) (1.7.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras==2.4.3) (3.13)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras==2.4.3) (3.1.0)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras==2.4.3) (1.21.6)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras==2.4.3) (1.5.2)\n",
            "Installing collected packages: keras\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.8.0\n",
            "    Uninstalling keras-2.8.0:\n",
            "      Successfully uninstalled keras-2.8.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.8.2+zzzcolab20220527125636 requires keras<2.9,>=2.8.0rc0, but you have keras 2.4.3 which is incompatible.\u001b[0m\n",
            "Successfully installed keras-2.4.3\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow==2.4.1\n",
            "  Downloading tensorflow-2.4.1-cp37-cp37m-manylinux2010_x86_64.whl (394.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 394.3 MB 13 kB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.1) (0.37.1)\n",
            "Collecting h5py~=2.10.0\n",
            "  Downloading h5py-2.10.0-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 51.0 MB/s \n",
            "\u001b[?25hCollecting gast==0.3.3\n",
            "  Downloading gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n",
            "Collecting typing-extensions~=3.7.4\n",
            "  Downloading typing_extensions-3.7.4.3-py3-none-any.whl (22 kB)\n",
            "Collecting grpcio~=1.32.0\n",
            "  Downloading grpcio-1.32.0-cp37-cp37m-manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 54.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.1) (1.6.3)\n",
            "Collecting tensorflow-estimator<2.5.0,>=2.4.0\n",
            "  Downloading tensorflow_estimator-2.4.0-py2.py3-none-any.whl (462 kB)\n",
            "\u001b[K     |████████████████████████████████| 462 kB 73.5 MB/s \n",
            "\u001b[?25hCollecting wrapt~=1.12.1\n",
            "  Downloading wrapt-1.12.1.tar.gz (27 kB)\n",
            "Collecting flatbuffers~=1.12.0\n",
            "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.1) (0.2.0)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.1) (1.1.0)\n",
            "Collecting numpy~=1.19.2\n",
            "  Downloading numpy-1.19.5-cp37-cp37m-manylinux2010_x86_64.whl (14.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 14.8 MB 48.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorboard~=2.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.1) (2.8.0)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.1) (1.15.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.1) (3.17.3)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.1) (1.1.2)\n",
            "Collecting absl-py~=0.10\n",
            "  Downloading absl_py-0.15.0-py3-none-any.whl (132 kB)\n",
            "\u001b[K     |████████████████████████████████| 132 kB 42.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.1) (3.3.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.1) (57.4.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.1) (1.8.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.1) (2.23.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.1) (3.4.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.1) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.1) (0.4.6)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.1) (1.35.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.1) (0.6.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.1) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.1) (4.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.1) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow==2.4.1) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow==2.4.1) (4.12.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.4->tensorflow==2.4.1) (3.8.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.1) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.1) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.1) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.1) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.1) (2.10)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow==2.4.1) (3.2.0)\n",
            "Building wheels for collected packages: wrapt\n",
            "  Building wheel for wrapt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wrapt: filename=wrapt-1.12.1-cp37-cp37m-linux_x86_64.whl size=68720 sha256=82425b0cf7f046f2e6e6c115f1d9e0507fcf53df79824149784a6277d1799ed6\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/76/4c/aa25851149f3f6d9785f6c869387ad82b3fd37582fa8147ac6\n",
            "Successfully built wrapt\n",
            "Installing collected packages: typing-extensions, numpy, grpcio, absl-py, wrapt, tensorflow-estimator, h5py, gast, flatbuffers, tensorflow\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing-extensions 4.1.1\n",
            "    Uninstalling typing-extensions-4.1.1:\n",
            "      Successfully uninstalled typing-extensions-4.1.1\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.21.6\n",
            "    Uninstalling numpy-1.21.6:\n",
            "      Successfully uninstalled numpy-1.21.6\n",
            "  Attempting uninstall: grpcio\n",
            "    Found existing installation: grpcio 1.47.0\n",
            "    Uninstalling grpcio-1.47.0:\n",
            "      Successfully uninstalled grpcio-1.47.0\n",
            "  Attempting uninstall: absl-py\n",
            "    Found existing installation: absl-py 1.1.0\n",
            "    Uninstalling absl-py-1.1.0:\n",
            "      Successfully uninstalled absl-py-1.1.0\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 1.14.1\n",
            "    Uninstalling wrapt-1.14.1:\n",
            "      Successfully uninstalled wrapt-1.14.1\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.8.0\n",
            "    Uninstalling tensorflow-estimator-2.8.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.8.0\n",
            "  Attempting uninstall: h5py\n",
            "    Found existing installation: h5py 3.1.0\n",
            "    Uninstalling h5py-3.1.0:\n",
            "      Successfully uninstalled h5py-3.1.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.5.3\n",
            "    Uninstalling gast-0.5.3:\n",
            "      Successfully uninstalled gast-0.5.3\n",
            "  Attempting uninstall: flatbuffers\n",
            "    Found existing installation: flatbuffers 2.0\n",
            "    Uninstalling flatbuffers-2.0:\n",
            "      Successfully uninstalled flatbuffers-2.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.8.2+zzzcolab20220527125636\n",
            "    Uninstalling tensorflow-2.8.2+zzzcolab20220527125636:\n",
            "      Successfully uninstalled tensorflow-2.8.2+zzzcolab20220527125636\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "xarray-einstats 0.2.2 requires numpy>=1.21, but you have numpy 1.19.5 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed absl-py-0.15.0 flatbuffers-1.12 gast-0.3.3 grpcio-1.32.0 h5py-2.10.0 numpy-1.19.5 tensorflow-2.4.1 tensorflow-estimator-2.4.0 typing-extensions-3.7.4.3 wrapt-1.12.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy",
                  "typing_extensions"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting keras_segmentation\n",
            "  Downloading keras_segmentation-0.3.0.tar.gz (23 kB)\n",
            "Requirement already satisfied: Keras>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from keras_segmentation) (2.4.3)\n",
            "Collecting imageio==2.5.0\n",
            "  Downloading imageio-2.5.0-py3-none-any.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 13.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: imgaug==0.2.9 in /usr/local/lib/python3.7/dist-packages (from keras_segmentation) (0.2.9)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from keras_segmentation) (4.6.0.66)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from keras_segmentation) (4.64.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from imageio==2.5.0->keras_segmentation) (1.19.5)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from imageio==2.5.0->keras_segmentation) (7.1.2)\n",
            "Requirement already satisfied: Shapely in /usr/local/lib/python3.7/dist-packages (from imgaug==0.2.9->keras_segmentation) (1.8.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from imgaug==0.2.9->keras_segmentation) (1.15.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from imgaug==0.2.9->keras_segmentation) (3.2.2)\n",
            "Requirement already satisfied: scikit-image>=0.11.0 in /usr/local/lib/python3.7/dist-packages (from imgaug==0.2.9->keras_segmentation) (0.18.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from imgaug==0.2.9->keras_segmentation) (1.7.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from Keras>=2.0.0->keras_segmentation) (3.13)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from Keras>=2.0.0->keras_segmentation) (2.10.0)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.11.0->imgaug==0.2.9->keras_segmentation) (2.6.3)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.11.0->imgaug==0.2.9->keras_segmentation) (1.3.0)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.11.0->imgaug==0.2.9->keras_segmentation) (2021.11.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->imgaug==0.2.9->keras_segmentation) (3.0.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->imgaug==0.2.9->keras_segmentation) (1.4.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->imgaug==0.2.9->keras_segmentation) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->imgaug==0.2.9->keras_segmentation) (2.8.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib->imgaug==0.2.9->keras_segmentation) (3.7.4.3)\n",
            "Building wheels for collected packages: keras-segmentation\n",
            "  Building wheel for keras-segmentation (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-segmentation: filename=keras_segmentation-0.3.0-py3-none-any.whl size=29071 sha256=5901a43d7d3fe7860278bf7baf64d23a597234bdd102c2694a75003725df2bd7\n",
            "  Stored in directory: /root/.cache/pip/wheels/68/dc/b3/596a3e2461ba16e935ef31661c26e823f841cfb577cec4c47a\n",
            "Successfully built keras-segmentation\n",
            "Installing collected packages: imageio, keras-segmentation\n",
            "  Attempting uninstall: imageio\n",
            "    Found existing installation: imageio 2.4.1\n",
            "    Uninstalling imageio-2.4.1:\n",
            "      Successfully uninstalled imageio-2.4.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed imageio-2.5.0 keras-segmentation-0.3.0\n"
          ]
        }
      ],
      "source": [
        "if USING_COLAB:\n",
        "    !pip install keras==2.4.3\n",
        "    !pip install tensorflow==2.4.1\n",
        "    !pip install keras_segmentation   \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CBtPjoc4wMnO"
      },
      "outputs": [],
      "source": [
        "from keras_segmentation.models.model_utils import transfer_weights\n",
        "from keras_segmentation.pretrained import pspnet_101_voc12\n",
        "from keras_segmentation.models.pspnet import pspnet_101"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-jX6Yb0wwMnP",
        "outputId": "15e69718-e962-4ad7-d4ac-2e3308ead0ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copying weights \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "412it [00:00, 782.57it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copied weights of 222 layers and skipped 1 layers\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "pretrained_model = pspnet_101_voc12()\n",
        "\n",
        "new_model = pspnet_101( n_classes=30 )\n",
        "\n",
        "transfer_weights( pretrained_model , new_model  ) # transfer weights from pre-trained model to your model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Paths"
      ],
      "metadata": {
        "id": "fw8yA4rEBtA0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from os import makedirs"
      ],
      "metadata": {
        "id": "h81cQbEptknv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tuple_path = ('data','processed','semantic_segmentation','unity_residential_interiors')\n",
        "\n",
        "train_images_path = path(*tuple_path, 'train_images')\n",
        "train_labels_path  = path(*tuple_path, 'train_labels')\n",
        "val_images_path = path(*tuple_path, 'val_images')\n",
        "val_labels_path  = path(*tuple_path, 'val_labels')\n",
        "\n",
        "model_path = path('models', 'pspnet_101', 'run_1')\n",
        "makedirs(model_path.remote, exist_ok=True)"
      ],
      "metadata": {
        "id": "hgrtxv7IsLUr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Configure training"
      ],
      "metadata": {
        "id": "Kh1uk-olB41X"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fBNyeJXRwMnQ",
        "outputId": "c8e5be51-b9f4-48aa-edd9-6ca39d886e37"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Verifying training dataset\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 700/700 [01:09<00:00, 10.11it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset verified! \n",
            "Verifying validation dataset\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 300/300 [00:30<00:00,  9.74it/s]\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset verified! \n",
            "Epoch 1/10\n",
            "  3/512 [..............................] - ETA: 1:16:02 - loss: 3.4916 - accuracy: 0.0166"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/imgaug/augmenters/segmentation.py:191: FutureWarning: skimage.measure.label's indexing starts from 0. In future version it will start from 1. To disable this warning, explicitely set the `start_label` parameter to 1.\n",
            "  segments = segmentation.slic(image, n_segments=n_segments_samples[i], compactness=10)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "512/512 [==============================] - 2979s 6s/step - loss: 3.4666 - accuracy: 0.0447 - val_loss: 3.6562 - val_accuracy: 0.0315\n",
            "saved  /drive/MyDrive/IA/seminario_innovacion/vigilancia_mascotas/models/pspnet_101/run_1/run_1.0\n",
            "Epoch 2/10\n",
            "512/512 [==============================] - 2882s 6s/step - loss: 3.1688 - accuracy: 0.1571 - val_loss: 3.4556 - val_accuracy: 0.0602\n",
            "saved  /drive/MyDrive/IA/seminario_innovacion/vigilancia_mascotas/models/pspnet_101/run_1/run_1.1\n",
            "Epoch 3/10\n",
            "512/512 [==============================] - 3069s 6s/step - loss: 2.9994 - accuracy: 0.2222 - val_loss: 3.2726 - val_accuracy: 0.1052\n",
            "saved  /drive/MyDrive/IA/seminario_innovacion/vigilancia_mascotas/models/pspnet_101/run_1/run_1.2\n",
            "Epoch 4/10\n",
            "512/512 [==============================] - 3062s 6s/step - loss: 2.8375 - accuracy: 0.2736 - val_loss: 3.1483 - val_accuracy: 0.1373\n",
            "saved  /drive/MyDrive/IA/seminario_innovacion/vigilancia_mascotas/models/pspnet_101/run_1/run_1.3\n",
            "Epoch 5/10\n",
            "497/512 [============================>.] - ETA: 1:24 - loss: 2.7508 - accuracy: 0.2925"
          ]
        }
      ],
      "source": [
        "new_model.train(\n",
        "    train_images = str(train_images_path.local),\n",
        "    train_annotations = str(train_labels_path.local),\n",
        "    validate=True,\n",
        "    val_images=str(val_images_path.local),\n",
        "    val_annotations=str(val_labels_path.local),\n",
        "    checkpoints_path = str(model_path.remote.joinpath('run_1')),\n",
        "    auto_resume_checkpoint=True,\n",
        "    epochs=10,\n",
        "    do_augment=True\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "?new_model.train"
      ],
      "metadata": {
        "id": "Uysbeh8_DD16"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svXI_bfKwMnR"
      },
      "source": [
        "## Update notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZYyxaPhtwMnR"
      },
      "outputs": [],
      "source": [
        "#update_notebook('to_remote')\n",
        "update_notebook()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.6.8 ('vigilancia_mascotas')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "601ef09c9bc66c05cb5add3ecbff44d038514508d2cd3dc8cd002c5ccc07f638"
      }
    },
    "colab": {
      "name": "1.0-vigilancia_mascotas-semantic_segmentation_retraining.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}